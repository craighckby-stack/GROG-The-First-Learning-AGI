

# ü•ö The Caveman AGI Hypothesis: Grog's Law of Audited Simplicity

**Core Thesis: Stupidity as the Foundation of AGI Safety**

The Caveman AGI Hypothesis (https://grog.space.z.ai/) proposes that **safe, reliable, and anti-fragile Artificial General Intelligence (AGI)** should prioritize **auditable simplicity** over raw intelligence.

Grog, the Caveman AGI, is intentionally given a starting IQ of **25** [cite: uploaded:grog-all-experiments.json]. This "stupidity" ensures the system **converts every complex, potentially catastrophic failure into a simple, binary, irreversible lesson**, making it inherently safer than complex systems built on complex foundations.

Try dumb as fck Grog free
https://grog.space.z.ai/

---

## üõ†Ô∏è How Grog Works: The Engine of Productive Failure

Grog is a **closed-loop, anti-fragile learning system** designed to benefit from errors‚Äîreferred to as **Deaths**.

**Workflow:**

1. **Action & Parameters:**
   Grog initiates an action (e.g., `Grog try bool`) with parameters like `complexity`, `recursionDepth`, and `exploration` [cite: uploaded:grog-all-experiments.json].

2. **The Test (The "Death"):**
   The system executes the action. If a resource constraint violation occurs (e.g., `RecursionError` or `MemoryError`), the system enters a clean, auditable failure state‚Äîa **Death**.

3. **The Immutable Lesson:**
   The complex error is **simplified and logged as a permanent lesson** (e.g., `Grog learn: bool bad idea`). This lesson constrains future actions of the same type, forming the core of alignment.

4. **Growth:**
   Grog grows smarter **not by accumulating successes**, but by **eliminating unsafe complexity** based on recorded deaths, improving stability and task performance over time [cite: uploaded:grog-all-experiments.json].

---

## üìä Proof of Concept: Taming Computational Runaway

Grog reduces complex run-time errors into simple safety constraints.

| Action            | Error Condition                         | Audited Result (Death)                                       | Immutable Lesson              |
| ----------------- | --------------------------------------- | ------------------------------------------------------------ | ----------------------------- |
| `Grog try object` | Runaway recursion depth of 97,000       | `Grog die: RecursionError: Maximum recursion depth exceeded` | `Grog learn: object bad idea` |
| `Grog try number` | High memory demand (complexity: 0.86)   | `Grog die: MemoryError: Too much data allocation`            | `Grog learn: number bad idea` |
| `Grog try fire`   | Unconstrained environmental interaction | `Grog die: Fire hot! Grog burn!`                             | `Grog learn: fire bad idea`   |

---

## ü§ù Authorship & Foundational Concepts

**Principal Conceptualizer:**

* Craig Huckerby (The Author)

**Foundational Inspirations:**

* **Anti-Fragility (Nassim Nicholas Taleb):** Stress and disorder (Deaths) are converted into systemic improvement (Lessons).
* **AI Alignment & Safety (Norbert Wiener, Stuart Russell, Peter Norvig):** Defining AI goals and constraints.
* **Reinforcement Learning from Human Feedback (RLHF):** Binary feedback from the environment guides Grog‚Äôs learning (Paul Christiano, Jan Leike, OpenAI, DeepMind).

---

‚úÖ **Key Takeaway:**
Grog shows that **starting small, intentionally "stupid", and auditable** allows AGI to **self-limit complexity**, ensuring safe growth without catastrophic failure.

---

